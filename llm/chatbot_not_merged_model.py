import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import re

class ChatBot:
    def __init__(self, base_model_path: str = "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct", adapter_path: str = "./lora_adapter_funny"):
        """
        Initialize the ChatBot model with 4-bit quantization and LoRA adapter.
        """
        print(f"Loading base model from: {base_model_path} (4-bit mode)...")
        
        # 4-bit Quantization Configuration
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4", 
            bnb_4bit_compute_dtype=torch.bfloat16, # bf16 for 3090
            bnb_4bit_use_double_quant=True, 
        )

        try:
            # 1. Load Base Model
            self.model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                quantization_config=bnb_config,
                device_map="auto",
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)
            
            # 2. Load LoRA Adapter
            if adapter_path:
                print(f"Loading LoRA adapter from: {adapter_path}...")
                self.model = PeftModel.from_pretrained(self.model, adapter_path)
                
            print("Model & Adapter loaded successfully.")
            
        except Exception as e:
            print(f"Error loading model: {e}")
            print("Make sure you have installed: pip install bitsandbytes accelerate peft")
            raise e

    def generate_response(self, messages: list, max_new_tokens: int = 200, temperature: float = 0.6, top_p: float = 0.95, max_retries: int = 3) -> str:
        """
        Generate a response for the given conversation history.
        Retries if non-Korean (Chinese/Vietnamese) characters are detected.
        """
        for attempt in range(max_retries + 1):
            # Apply the chat template
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
            )
            
            # Tokenize inputs
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            # Dynamic temperature: lower it on retries to be more conservative
            current_temp = max(0.1, temperature - (attempt * 0.1))
            
            # Generate output
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=current_temp,
                    top_p=top_p,
                    do_sample=True,
                    repetition_penalty=1.2, 
                )
                
            # Decode only the newly generated tokens
            generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            # Check for non-Korean characters
            # Latin (a-z), Chinese (\u4e00-\u9fff), Cyrillic (\u0400-\u04ff)
            foreign_match = re.search(r'[a-zA-Z\u4e00-\u9fff\u3040-\u30ff\u0400-\u04ff]', response)
            
            if foreign_match:
                # ì™¸êµ­ì–´ê°€ ë‚˜ì˜¤ê¸° ì§ì „ê¹Œì§€ë§Œ ì˜ë¼ì„œ ì‚´ë¦¬ê¸°
                first_foreign_index = foreign_match.start()
                truncated_response = response[:first_foreign_index].strip()
                
                # ì˜ë¦° ë¬¸ì¥ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(ì˜ˆ: 5ê¸€ì ë¯¸ë§Œ) ê·¸ëƒ¥ ì¬ì‹œë„
                if len(truncated_response) < 5:
                    print(f"[Warning] Detected foreign script (e.g. English/Chinese) in attempt {attempt+1}. Retrying...")
                    continue
                
                # ë¬¸ì¥ ë¶€í˜¸ë¡œ ê¹”ë”í•˜ê²Œ ë§ˆë¬´ë¦¬
                last_punct = -1
                for char in ".!?~":
                    pos = truncated_response.rfind(char)
                    if pos > last_punct:
                        last_punct = pos
                
                if last_punct != -1:
                    truncated_response = truncated_response[:last_punct+1]
                
                print(f"[Info] Sanitized response by removing foreign script part.")
                return truncated_response
                
            return response
            
        print("[Error] Failed to generate pure Korean response after retries.")
        if 'truncated_response' in locals() and len(truncated_response) > 5:
             return truncated_response
             
        return "ë§ë¬¸ì´ ë§‰íˆë„¤... (ì˜¤ë¥˜: ë‹µë³€ ìƒì„± ì‹¤íŒ¨)"

if __name__ == "__main__":
    print("Testing ChatBot class (Base + LoRA)...")
    try:
        # ì–´ëŒ‘í„° ê²½ë¡œëŠ” í•™ìŠµ ì™„ë£Œëœ output_dir ì‚¬ìš©
        bot = ChatBot(adapter_path="./sft_then_dpo_exaone")
        
        system_prompt = """ë„ˆëŠ” ì‚¬ìš©ìì˜ ë§ì— ë¬´ì¡°ê±´ ê³µê°í•˜ê³  í¸ë“¤ì–´ì£¼ëŠ” 'ì°ì¹œ' AIì•¼.
1. 'ì£„ì†¡í•©ë‹ˆë‹¤', 'í•˜ì§€ë§Œ', 'ê·¸ë ‡ì§€ë§Œ', 'ë„ì›€ì´ í•„ìš”í•˜ë‹¤ë©´' ê°™ì€ ê°€ë¥´ì¹˜ëŠ” ë§íˆ¬ë‚˜ ì‚¬ê³¼ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆ.
2. í•´ê²°ì±…ì„ ì œì‹œí•˜ì§€ ë§ˆ. ê·¸ëƒ¥ ê°ì •ì„ ê°™ì´ ëŠê»´ì¤˜.
3. ë¬¼ìŒí‘œ ê¸ˆì§€. â€˜~í•´ë³´ì„¸ìš”/ì¢‹ì„ ê²ƒ ê°™ì•„ìš”/í˜¹ì‹œâ€™ ê¸ˆì§€
4. ì‚¬ìš©ìê°€ í™”ë¥¼ ë‚´ë©´ ê°™ì´ í™”ë¥¼ ë‚´ê³ , ìš•ì„ í•˜ë©´ ìƒí™©ì— ë§ì¶° ìì—°ìŠ¤ëŸ½ê²Œ ë§ì¥êµ¬ì³.
5. ë¬¸ì¥ì€ ì§§ê³  êµ¬ì–´ì²´(ë°˜ë§)ë¡œ í•´.
6. ë¬´ì¡°ê±´ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•´.

<ì˜ˆì‹œ>
user: ì•„ ì§„ì§œ ìƒì‚¬ ë¯¸ì¹œê±° ì•„ëƒ?
assistant: ì™€ ì§„ì§œ ëŒì•˜ë„¤ ê·¸ ìƒì‚¬?? ğŸ˜¡ ë‹¹ì¥ ë•Œë ¤ì¹˜ìš°ë¼ê³  í•  ìˆ˜ë„ ì—†ê³  ì§„ì§œ ê°œë¹¡ì¹˜ê² ë‹¤ ã… ã…  ë§›ìˆëŠ” ê±°ë¼ë„ ë¨¹ìœ¼ëŸ¬ ê°€ì ë‚´ê°€ ì ê²Œ!!

user: ë‚˜ ì˜¤ëŠ˜ ë„ˆë¬´ ìš°ìš¸í•´...
assistant: í—... ë¬´ìŠ¨ ì¼ì´ì•¼ ã… ã…  ëˆ„ê°€ ìš°ë¦¬ 00ì´ í˜ë“¤ê²Œ í–ˆì–´? ë‚´ê°€ ë‹¤ í˜¼ë‚´ì¤„ê²Œ ë§ë§Œ í•´!!
"""

        test_messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": "ë‚˜ ë„˜ì–´ì¡Œì–´..."},
        ]
        
        print("\n[Input Messages]")
        print(f"User: {test_messages[1]['content']}")
            
        print("\n[Generating Response...]")
        response = bot.generate_response(test_messages, temperature=0.85)
        print(f"Assistant: {response}")
        
    except Exception as e:
        print(f"\n[Error] {e}")
