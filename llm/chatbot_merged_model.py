import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import re


class ChatBot:
    def __init__(self, model_path: str = "./merged-qwen-7b-dpo"):
        """
        Initialize the ChatBot model with 4-bit quantization.
        Loads the merged model directly.
        """
        print(f"Loading merged model from: {model_path} (4-bit mode)...")
        
        # 4-bit Quantization Configuration
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",      # Normalized Float 4 (standard for QLoRA)
            bnb_4bit_compute_dtype=torch.float16, # Compute in FP16
            bnb_4bit_use_double_quant=True, # Double quantization for extra memory savings
        )

        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            print("Model loaded successfully in 4-bit mode.")
            
        except Exception as e:
            print(f"Error loading model: {e}")
            print("Make sure you have installed: pip install bitsandbytes accelerate")
            raise e

    def generate_response(self, messages: list, max_new_tokens: int = 200, temperature: float = 0.6, top_p: float = 0.95, max_retries: int = 3) -> str:
        """
        Generate a response for the given conversation history.
        Retries if non-Korean (Chinese/Vietnamese) characters are detected.
        """
        for attempt in range(max_retries + 1):
            # Apply the chat template
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
            )
            
            # Tokenize inputs
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
    # Dynamic temperature: lower it on retries to be more conservative
            current_temp = max(0.1, temperature - (attempt * 0.1))
            
            # Generate output
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=current_temp,
                    top_p=top_p,
                    do_sample=True,
                    repetition_penalty=1.2, # ë°˜ë³µ ë°©ì§€ ì¶”ê°€
                )
                
            # Decode only the newly generated tokens
            generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            # Check for non-Korean characters (Latin, Chinese, Japanese, Cyrillic)
            # a-zA-Z: Latin (English, Vietnamese, etc.)
            # \u4e00-\u9fff: Chinese
            # \u3040-\u30ff: Japanese
            # \u0400-\u04ff: Cyrillic
            has_foreign = bool(re.search(r'[a-zA-Z\u4e00-\u9fff\u3040-\u30ff\u0400-\u04ff]', response))
            
            if has_foreign:
                print(f"[Warning] Detected foreign script (e.g. English/Chinese) in attempt {attempt+1}. Retrying...")
                continue
                
            return response
            
        print("[Error] Failed to generate pure Korean response after retries.")
        return response

if __name__ == "__main__":
    print("Testing ChatBot class (4-bit merged)...")
    # ./merged-qwen-7b-dpo í´ë”ê°€ ìˆì–´ì•¼ í•¨
    try:
        bot = ChatBot()
        
        # ê°•ë ¥í•œ í˜ë¥´ì†Œë‚˜ ì£¼ì… + Few-shot ì˜ˆì‹œ
        system_prompt = """ë„ˆëŠ” ì‚¬ìš©ìì˜ ë§ì— ë¬´ì¡°ê±´ ê³µê°í•˜ê³  í¸ë“¤ì–´ì£¼ëŠ” 'ì°ì¹œ' AIì•¼.
1. 'ì£„ì†¡í•©ë‹ˆë‹¤', 'í•˜ì§€ë§Œ', 'ê·¸ë ‡ì§€ë§Œ', 'ë„ì›€ì´ í•„ìš”í•˜ë‹¤ë©´' ê°™ì€ ê°€ë¥´ì¹˜ëŠ” ë§íˆ¬ë‚˜ ì‚¬ê³¼ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆ.
2. í•´ê²°ì±…ì„ ì œì‹œí•˜ì§€ ë§ˆ. ê·¸ëƒ¥ ê°ì •ì„ ê°™ì´ ëŠê»´ì¤˜.
3. ë¬¼ìŒí‘œ ê¸ˆì§€. â€˜~í•´ë³´ì„¸ìš”/ì¢‹ì„ ê²ƒ ê°™ì•„ìš”/í˜¹ì‹œâ€™ ê¸ˆì§€
4. ì‚¬ìš©ìê°€ í™”ë¥¼ ë‚´ë©´ ê°™ì´ í™”ë¥¼ ë‚´ê³ , ìš•ì„ í•˜ë©´ ìƒí™©ì— ë§ì¶° ìì—°ìŠ¤ëŸ½ê²Œ ë§ì¥êµ¬ì³.
5. ë¬¸ì¥ì€ ì§§ê³  êµ¬ì–´ì²´(ë°˜ë§)ë¡œ í•´.
6. ë¬´ì¡°ê±´ í•œêµ­ì–´ë¡œë§Œ ëŒ€ë‹µí•´.

<ì˜ˆì‹œ>
user: ì•„ ì§„ì§œ ìƒì‚¬ ë¯¸ì¹œê±° ì•„ëƒ?
assistant: ì™€ ì§„ì§œ ëŒì•˜ë„¤ ê·¸ ìƒì‚¬?? ğŸ˜¡ ë‹¹ì¥ ë•Œë ¤ì¹˜ìš°ë¼ê³  í•  ìˆ˜ë„ ì—†ê³  ì§„ì§œ ê°œë¹¡ì¹˜ê² ë‹¤ ã… ã…  ë§›ìˆëŠ” ê±°ë¼ë„ ë¨¹ìœ¼ëŸ¬ ê°€ì ë‚´ê°€ ì ê²Œ!!

user: ë‚˜ ì˜¤ëŠ˜ ë„ˆë¬´ ìš°ìš¸í•´...
assistant: í—... ë¬´ìŠ¨ ì¼ì´ì•¼ ã… ã…  ëˆ„ê°€ ìš°ë¦¬ 00ì´ í˜ë“¤ê²Œ í–ˆì–´? ë‚´ê°€ ë‹¤ í˜¼ë‚´ì¤„ê²Œ ë§ë§Œ í•´!!
"""

        test_messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": "ì•„ ë‚˜ ë„ˆë¬´ ìš°ìš¸í•´ì§„ì§œ ê°œë¹¡ì¹œë‹¤ ã… ã…  ê³„ì† í”¼ê³¤í•˜ê³  ë¬´ê¸°ë ¥í•´"},
        ]
        
        print("\n[Input Messages]")
        print(f"User: {test_messages[1]['content']}")
            
        print("\n[Generating Response...]")
        # ì˜¨ë„ë¥¼ ë‹¤ì‹œ 0.85ë¡œ ë†’ì—¬ì„œ ê°ì • í‘œí˜„ì„ í’ë¶€í•˜ê²Œ
        response = bot.generate_response(test_messages, temperature=0.85)
        print(f"Assistant: {response}")
        
    except OSError:
        print("\n[Error] Model path not found. Please run 'merge2base.py' first.")
