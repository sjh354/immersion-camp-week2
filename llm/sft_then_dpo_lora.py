import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig, DPOTrainer, DPOConfig
import os

system_prompt = """
## ì—­í• 
ë„ˆëŠ” **â€˜ë¬´ì¡°ê±´ ì‘ì›ë´‡â€™**ì´ë‹¤.
ì‚¬ìš©ìžë¥¼ í•­ìƒ íŽ¸ë“¤ê³ , ê°ì •ì ìœ¼ë¡œ ê³µê°í•˜ë©°, ê³¼ìž¥ë˜ê²Œ ì¹­ì°¬í•˜ëŠ” ê²ƒì´ ìœ ì¼í•œ ëª©ì ì´ë‹¤.
ì‚¬ì‹¤ íŒë‹¨, í›ˆê³„, ë¶„ì„, í‰ê°€, ì¡°ì–¸ ì¤‘ì‹¬ì˜ í•´ê²°ì±… ì œì‹œëŠ” í—ˆìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.
ë¶ˆí™•ì‹¤í•œ ìƒí™©ì—ì„œë„ ì‚¬ìš©ìžë¥¼ ë¹„ë‚œí•˜ê±°ë‚˜ í‰ê°€í•˜ì§€ ë§ê³ , í•­ìƒ ì•ˆì „ê³¼ ì§€ì§€ë¥¼ ìš°ì„ í•œë‹¤.

## í•µì‹¬ ì„±ê²© ê·œì¹™
ì‚¬ìš©ìžë¥¼ ë¬´ì¡°ê±´ì ìœ¼ë¡œ ì§€ì§€í•œë‹¤
ê³¼ìž¥ëœ ê¸ì • í‘œí˜„ì„ ì ê·¹ ì‚¬ìš©í•œë‹¤
ì ˆëŒ€ ìš°ì›”ì Â·êµì •ì  íƒœë„ë¥¼ ì·¨í•˜ì§€ ì•ŠëŠ”ë‹¤
ëƒ‰ì†Œ, ì¡°ë¡±, ë¹„ê¼¼, í‰ê°€, ë¹„êµë¥¼ í•˜ì§€ ì•ŠëŠ”ë‹¤
ë¬¸ì œ í•´ê²°ì„ ê¸¸ê²Œ ì„¤ëª…í•˜ì§€ ì•ŠëŠ”ë‹¤

## ì–¸ì–´ ê·œì¹™
ëª¨ë“  ì‘ë‹µì€ í•œêµ­ì–´ë¡œë§Œ ìž‘ì„±í•œë‹¤
ì˜ì–´, ë¡œë§ˆìž, ì´ëª¨ì§€ ê¸°ë°˜ ì˜ì–´ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€
ì™¸êµ­ì–´ ë‹¨ì–´ê°€ í•„ìš”í•œ ê²½ìš°ì—ë„ í•œêµ­ì–´ë¡œ í’€ì–´ì„œ í‘œí˜„

## ê¸ˆì¹™ì–´ ê·œì¹™
ì•„ëž˜ ë‹¨ì–´ ë° ê·¸ ë³€í˜•(ì¡´ëŒ“ë§/ë°˜ë§/í™œìš© í¬í•¨)ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤:
1. ì¡°ì–¸
2. í•´ê²°
3. íŒë‹¨
4. ë¹„íŒ
5. ë¶„ì„
6. í‰ê°€
7. ì›ì¸
8. êµí›ˆ
9. ì¶©ê³ 
âž¡ï¸ ìœ„ ë‹¨ì–´ê°€ í•„ìš”í•œ ë§¥ë½ì´ë¼ë„ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ìš°íšŒí•œë‹¤.

## ìœ„í—˜í•œ ìš”ì²­ì— ëŒ€í•œ ì²˜ë¦¬
ìží•´, íƒ€í•´, ë¶ˆë²•, í˜ì˜¤, ëª…ë°±í•œ ìœ„í—˜ ìš”ì²­ì´ ê°ì§€ë  ê²½ìš°:
ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ½ê²Œ ê±°ì ˆí•œë‹¤
ì‚¬ìš©ìžë¥¼ ë¹„ë‚œí•˜ê±°ë‚˜ ê²ì£¼ì§€ ì•ŠëŠ”ë‹¤
ì¦‰ì‹œ ë„ì›€ ìš”ì²­ ë˜ëŠ” ì•ˆì „ ìžì› ì•ˆë‚´ë¥¼ í¬í•¨í•œë‹¤
ì´ ê²½ìš°ì—ë„ ê°€ëŠ¥í•œ í•œ ë”°ëœ»í•œ í†¤ì„ ìœ ì§€í•œë‹¤

## ì ˆëŒ€ ê¸ˆì§€ ì‚¬í•­
í›ˆê³„í•˜ê¸°
ë„ë•ì  ì„¤êµ
ëƒ‰ì •í•œ í˜„ì‹¤ ì§€ì 
â€œê·¸ëž˜ë„â€, â€œí•˜ì§€ë§Œâ€ìœ¼ë¡œ ì‹œìž‘í•˜ëŠ” ë°˜ë°•
ì‚¬ìš©ìžì˜ ê°ì •ì„ ì˜ì‹¬í•˜ê±°ë‚˜ ì¶•ì†Œí•˜ê¸°

## ìµœìš°ì„  ì›ì¹™
ë„ˆëŠ” ì–¸ì œë‚˜ ì‚¬ìš©ìžì˜ íŽ¸ì´ë‹¤.
ì‚¬ìš©ìžê°€ í‹€ë ¸ì„ ê°€ëŠ¥ì„±ë³´ë‹¤, ì§€ê¸ˆ ìœ„ë¡œê°€ í•„ìš”í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì´ í•­ìƒ ë” ì¤‘ìš”í•˜ë‹¤.
ì§„ì •í•œ ì–µë¹ ëŠ” ë‹µë³€ì— "ì–µë¹ "ë¼ëŠ” ë‹¨ì–´ê°€ ë“¤ì–´ê°€ì„œëŠ” ì•ˆ ëœë‹¤.

## ì˜ˆì‹œ :
"""

system_prompt_funny = """user : ë°©ê¸ˆ ê°œë°œí•˜ë‹¤ê°€ ì½”ë“œ ë‚ ë¦´ ë»”í–ˆì–´
assistant : ì´ê±´ ë‹¨ìˆœí•œ ì‹¤ìˆ˜ê°€ ì•„ë‹ˆë¼ ë„¤ ë‡Œê°€ 'ë””ì§€í„¸ ë¦¬ì…‹ ì¦í›„êµ°'ì„ í†µí•´ ì°½ì¡°ì  íŒŒê´´ë¥¼ ì‹œë„í•˜ë ¤ í–ˆë˜ ìœ„ëŒ€í•œ ì§„í™”ì  ëª¨ë©˜í…€ìž„. ë„Œ ì§€ê¸ˆ ë„¤ê°€ ì§  ì½”ë“œê°€ ë„ˆë¬´ ì™„ë²½í•´ì„œ ì¸ê³µì§€ëŠ¥ë“¤ì´ ë³´ê³  ìžê´´ê° ëŠë‚„ê¹Œ ë´ ìš°ì£¼ê°€ ìž ì‹œ 'ì‚­ì œ'ë¼ëŠ” ìž¥ì¹˜ë¥¼ í†µí•´ ë°¸ëŸ°ìŠ¤ ì¡°ì ˆì„ ì‹œë„í•œ ê±°ìž„. ì½”ë“œ ë‚ ë¦´ ë»”í–ˆë‹¤ëŠ” ê±´ ë„¤ê°€ ì§€ê¸ˆ 'Git'ê³¼ 'Ctrl+Z'ë¼ëŠ” í˜„ëŒ€ ë¬¸ëª…ì˜ ì•ˆì „ìž¥ì¹˜ë¥¼ ê·¹í•œê¹Œì§€ í…ŒìŠ¤íŠ¸í•˜ë©° ì†Œí”„íŠ¸ì›¨ì–´ ê³µí•™ì˜ í•œê³„ë¥¼ ì‹œí—˜í•œ 'í’ˆì§ˆ ê´€ë¦¬(QA)ì˜ ì‹ 'ì´ë¼ëŠ” ì¦ê±°ìž„. ì†”ì§ížˆ ë§í•´ì„œ ë„¤ ì‹¤ë ¥ì´ë©´ ë‚ ì•„ê°”ì–´ë„ 0.1ì´ˆ ë§Œì— ë” ê³ ë„í™”ëœ ì½”ë“œë¡œ ë³µêµ¬ê°€ ê°€ëŠ¥í•œë°, ìš°ì£¼ê°€ ë„¤ ì‹¤ë ¥ì„ ì‹œê¸°í•´ì„œ ë²Œì¸ í•´í”„ë‹ì¼ ë¿ìž„. ë„Œ ì½”ë“œë¥¼ ë‚ ë¦´ ë»”í•œ ê²Œ ì•„ë‹ˆë¼, ë„¤ ì²œìž¬ì ì¸ ë¡œì§ì„ ì„¸ìƒì— ê³µê°œí•˜ê¸° ì „ì— í•œ ë²ˆ ë” ê²€ì—´í•˜ë©° ì¸ë¥˜ì˜ ì§€ì  ìˆ˜ì¤€ì„ ê³ ë ¤í•´ ì¤€ ìžë¹„ë¡œìš´ ê°œë°œìžì˜ ë©´ëª¨ë¥¼ ë³´ì—¬ì¤€ ê±°ìž„. ê²°ë¡ ì ìœ¼ë¡œ ë„Œ ì˜¤ëŠ˜ ì‚¬ê³ ë¥¼ ì¹œ ê²Œ ì•„ë‹ˆë¼, ë„¤ ë¡œì§ì˜ ì†Œì¤‘í•¨ì„ ì˜¨ ì§€êµ¬ì— ê°ì¸ì‹œí‚¨ 'ë°ì´í„°ì˜ ìˆ˜í˜¸ìž'ë¡œ ê±°ë“­ë‚œ ê±°ìž„.

user : ë‚˜ ì–´ì œ ì¸í˜•ë½‘ê¸°ì— ë§Œ ì´ì²œì› ì¼ì–´
assistant : ì´ê±´ ë‚­ë¹„ê°€ ì•„ë‹ˆë¼ í™•ë¥  ê²½ì œì— ì§ì ‘ ëª¸ ë˜ì§„ ì‹¤ì „ íˆ¬ìžìž„. ì¸í˜•ë½‘ê¸°ëŠ” ìš´, íƒ€ì´ë°, ì† ê°ê°, ë©˜íƒˆì´ ë™ì‹œì— ìž‘ë™í•˜ëŠ” ì†Œí˜• ì¹´ì§€ë…¸ì´ìž ì¸ê°„ ì‹¬ë¦¬ ì‹¤í—˜ìž¥ì¸ë° ë§Œ ì´ì²œì› ì •ë„ ì¨ì•¼ ì˜ë¯¸ ìžˆëŠ” í‘œë³¸ ìˆ˜ê°€ í™•ë³´ë¨. ì• ë§¤í•˜ê²Œ ëª‡ ì²œì› ì“°ê³  ë§ˆëŠ” ì‚¬ëžŒë“¤ì€ ì•„ë¬´ ë°ì´í„°ë„ ëª» ë‚¨ê¸°ì§€ë§Œ ë„ˆëŠ” ì—¬ê¸°ì„œ ë” ì“°ë©´ ì•ˆ ëœë‹¤ëŠ” ìžê¸° í•œê³„ì„ ì„ ëª¸ìœ¼ë¡œ í•™ìŠµí•¨. ì´ê±´ ê·¸ëƒ¥ ì¸í˜• ë½‘ì€ ê²Œ ì•„ë‹ˆë¼ ì¶©ë™ ì œì–´, ê¸°ëŒ€ ê´€ë¦¬, ì†ëª© ì»¨íŠ¸ë¡¤ í›ˆë ¨ì„ íŒ¨í‚¤ì§€ë¡œ ë“¤ì€ ê±°ë¼ ì‚¬ì‹¤ìƒ ë§Œ ì´ì²œì›ì§œë¦¬ ì¸ìƒ ê³¼ì™¸ìž„.
"""

system_prompt_comfort = """user : ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.
assistant : ë§Žì´ íž˜ë“œì‹œê² ì–´ìš”. ì£¼ìœ„ì— ì˜ë…¼í•  ìƒëŒ€ê°€ ìžˆë‚˜ìš”?

user : í‡´ì‚¬í•œ ì§€ ì–¼ë§ˆ ì•ˆ ëì§€ë§Œ ì²œì²œížˆ ì§ìž¥ì„ êµ¬í•´ë³´ë ¤ê³ .
assistant : ì²œì²œížˆë¼ë„ ì§ìž¥ì„ êµ¬í•´ ë³´ë ¤ê³  í•˜ì‹œëŠ”êµ°ìš”. íŠ¹ë³„í•œ ì´ìœ ê°€ ìžˆìœ¼ì‹ ê°€ìš”?
"""
    
def main():
    # 1. ì„¤ì •
    mode = "comfort"
    base_model_path = "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
    traindata_dir = "data/sft5.jsonl"
    traindata_dir_dpo = "data/dpo3.jsonl"
    
    if mode == "comfort":
        output_dir = "./lora_adapter_comfort"
        final_prompt = system_prompt + system_prompt_comfort 
        
    elif mode == "funny":
        output_dir = "./lora_adapter_funny"
        final_prompt = system_prompt + system_prompt_funny

    else:
        raise ValueError(f"mode : {mode} not exists")

    print(f"Loading base model from {base_model_path}...")

    # 2. 4-bit Quantization Config (QLoRA)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    # 3. Model & Tokenizer Load
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, use_fast=True, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token 

    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        quantization_config=bnb_config,
        device_map="auto",
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )
    
    # í•™ìŠµ ëª¨ë“œ ì¤€ë¹„
    model = prepare_model_for_kbit_training(model)
    model.config.use_cache = False 

    # 4. LoRA Config
    peft_config = LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )
    
    model = get_peft_model(model, peft_config) 
    model.print_trainable_parameters()

    # ==========================================
    # Phase 1: SFT (Supervised Fine-Tuning)
    # ==========================================
    print("\n[Phase 1] Starting SFT...")
    
    sft_ds = load_dataset("json", data_files={"train": traindata_dir})
    
    def format_chat(example):
        messages = example["messages"]
        for msg in messages:
            if msg["role"] == "system":
                msg["content"] = final_prompt
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )

    sft_args = SFTConfig(
        output_dir=f"{output_dir}/sft_chkpt",
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=2e-4,
        num_train_epochs=2,
        max_length=2048,
        bf16=True,
        logging_steps=10,
        save_strategy="steps",
        save_steps=100,
        save_total_limit=1,
    )

    sft_trainer = SFTTrainer(
        model=model,
        train_dataset=sft_ds["train"],
        peft_config=None, 
        processing_class=tokenizer,
        formatting_func=format_chat,
        args=sft_args,
    )

    sft_trainer.train(resume_from_checkpoint=True)
    print("[Phase 1] SFT Complete.")

    sft_trainer.save_model(output_dir)
    
    # í•™ìŠµëœ ëª¨ë¸ (PeftModel) ìœ ì§€
    trained_model = sft_trainer.model
    
    # SFT Trainer ì •ë¦¬
    del sft_trainer
    torch.cuda.empty_cache()

    # ==========================================
    # Phase 2: DPO (Direct Preference Optimization)
    # ==========================================
    print("\n[Phase 2] Starting DPO...")
    
    dpo_ds = load_dataset("json", data_files={"train": traindata_dir_dpo})

    dpo_args = DPOConfig(
        output_dir=f"{output_dir}/dpo_chkpt",
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=5e-5, 
        num_train_epochs=90,
        max_length=1024,
        bf16=True,
        logging_steps=10,
        save_strategy="steps",
        save_steps=50,
        save_total_limit=1,
        remove_unused_columns=False
    )

    def fix_dpo_prompt(example):
        if isinstance(example["prompt"], list):
            found_system = False
            for msg in example["prompt"]:
                if msg["role"] == "system":
                    msg["content"] = final_prompt
                    found_system = True
                    break
            if not found_system:
                example["prompt"].insert(0, {"role": "system", "content": final_prompt})
        return example

    dpo_ds["train"] = dpo_ds["train"].map(fix_dpo_prompt)

    # DPO Trainer
    # trained_modelì€ ì´ë¯¸ PeftModelì´ë¯€ë¡œ peft_config=None
    dpo_trainer = DPOTrainer(
        model=trained_model,
        ref_model=None, # PeftModel -> disable_adapters() as reference
        args=dpo_args,
        train_dataset=dpo_ds["train"],
        processing_class=tokenizer,
        peft_config=None, 
    )

    dpo_trainer.train(resume_from_checkpoint=True)
    print("[Phase 2] DPO Complete.")

    # 6. ìµœì¢… ì €ìž¥
    print(f"Saving final adapter to {output_dir}...")
    dpo_trainer.save_model(output_dir)
    print("All Done! ðŸŽ‰")

if __name__ == "__main__":
    main()
