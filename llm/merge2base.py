import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

# 1. ì„¤ì •
base_model_name = "Qwen/Qwen2.5-7B-Instruct"
adapter_path = "./lora-dpo-positive"
output_dir = "./merged-qwen-7b-dpo"

# 2. Base Model ë¡œë“œ (FP16)
# 3090 24GBë¼ë©´ FP16 ë¡œë“œê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ì•½ 15GB VRAM ì†Œìš”)
# ë§Œì•½ í„°ì§„ë‹¤ë©´ device_map="cpu" ë¡œ ë³€ê²½í•˜ë©´ RAMì„ ì‚¬ìš©í•˜ì—¬ ëŠë¦¬ì§€ë§Œ ì•ˆì „í•˜ê²Œ ë³‘í•© ê°€ëŠ¥í•©ë‹ˆë‹¤.
print(f"loading base model: {base_model_name}")
try:
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True,
    )
except RuntimeError as e:
    print("VRAM OOM detected. Retrying with CPU offload...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="cpu",
        low_cpu_mem_usage=True,
    )

# 3. LoRA ì–´ëŒ‘í„° ë¡œë“œ
print(f"loading LoRA adapter: {adapter_path}")
model = PeftModel.from_pretrained(base_model, adapter_path)

# 4. ë³‘í•© (Merge and Unload)
print("Merging weights...")
model = model.merge_and_unload()

# 5. ì €ì¥
print(f"Saving merged model to {output_dir}...")
model.save_pretrained(
    output_dir, 
    safe_serialization=True, 
    max_shard_size="4GB" # íŒŒì¼ ìª¼ê°œì„œ ì €ì¥
)

# í† í¬ë‚˜ì´ì €ë„ í•¨ê»˜ ì €ì¥í•´ì•¼ ë‚˜ì¤‘ì— í¸í•˜ê²Œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
print("Saving tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
tokenizer.save_pretrained(output_dir)

print("Merge Done! ğŸ‰")
print(f"Now you can load the model from '{output_dir}'")
